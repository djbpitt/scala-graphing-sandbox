package net.collatex.util

import net.collatex.reptilian.TokenEnum.Token

val data: List[List[Token]] = List(
  List(Token("vegetable ", "vegetable", 0, 1124), Token("poisons", "poisons", 0, 1125), Token(". ", ".", 0, 1126)),
  List(Token("vegetable ", "vegetable", 1, 13973), Token("poisons", "poisons", 1, 13974), Token(". ", ".", 1, 13975)),
  List(
    Token("vegetable ", "vegetable", 2, 26894),
    Token("poisons", "poisons", 2, 26895),
    Token(": ", ":", 2, 26896),
    Token("Professor ", "professor", 2, 26897),
    Token("Wyman ", "wyman", 2, 26898),
    Token("has ", "has", 2, 26899),
    Token("recently ", "recently", 2, 26900),
    Token("communicated ", "communicated", 2, 26901),
    Token("to ", "to", 2, 26902),
    Token("me ", "me", 2, 26903),
    Token("a ", "a", 2, 26904),
    Token("good ", "good", 2, 26905),
    Token("illustration ", "illustration", 2, 26906),
    Token("of ", "of", 2, 26907),
    Token("this ", "this", 2, 26908),
    Token("fact", "fact", 2, 26909),
    Token("; ", ";", 2, 26910),
    Token("on ", "on", 2, 26911),
    Token("asking ", "asking", 2, 26912),
    Token("some ", "some", 2, 26913),
    Token("farmers ", "farmers", 2, 26914),
    Token("in ", "in", 2, 26915),
    Token("Florida ", "florida", 2, 26916),
    Token("how ", "how", 2, 26917),
    Token("it ", "it", 2, 26918),
    Token("was ", "was", 2, 26919),
    Token("that ", "that", 2, 26920),
    Token("all ", "all", 2, 26921),
    Token("their ", "their", 2, 26922),
    Token("pigs ", "pigs", 2, 26923),
    Token("were ", "were", 2, 26924),
    Token("black", "black", 2, 26925),
    Token(", ", ",", 2, 26926),
    Token("they ", "they", 2, 26927),
    Token("informed ", "informed", 2, 26928),
    Token("him ", "him", 2, 26929),
    Token("that ", "that", 2, 26930),
    Token("the ", "the", 2, 26931),
    Token("pigs ", "pigs", 2, 26932),
    Token("ate ", "ate", 2, 26933),
    Token("the ", "the", 2, 26934),
    Token("paint", "paint", 2, 26935),
    Token("-", "-", 2, 26936),
    Token("root ", "root", 2, 26937),
    Token("(", "(", 2, 26938),
    Token("Lachnanthes", "lachnanthes", 2, 26939),
    Token(")", ")", 2, 26940),
    Token(", ", ",", 2, 26941),
    Token("which ", "which", 2, 26942),
    Token("coloured ", "coloured", 2, 26943),
    Token("their ", "their", 2, 26944),
    Token("bones ", "bones", 2, 26945),
    Token("pink", "pink", 2, 26946),
    Token(", ", ",", 2, 26947),
    Token("and ", "and", 2, 26948),
    Token("which ", "which", 2, 26949),
    Token("caused ", "caused", 2, 26950),
    Token("the ", "the", 2, 26951),
    Token("hoofs ", "hoofs", 2, 26952),
    Token("of ", "of", 2, 26953),
    Token("all ", "all", 2, 26954),
    Token("but ", "but", 2, 26955),
    Token("the ", "the", 2, 26956),
    Token("black ", "black", 2, 26957),
    Token("varieties ", "varieties", 2, 26958),
    Token("to ", "to", 2, 26959),
    Token("drop ", "drop", 2, 26960),
    Token("off", "off", 2, 26961),
    Token("; ", ";", 2, 26962),
    Token("and ", "and", 2, 26963),
    Token("one ", "one", 2, 26964),
    Token("of ", "of", 2, 26965),
    Token("the ", "the", 2, 26966),
    Token("\"", "\"", 2, 26967),
    Token("crackers", "crackers", 2, 26968),
    Token("\" ", "\"", 2, 26969),
    Token("( ", "(", 2, 26970),
    Token("i", "i", 2, 26971),
    Token(".", ".", 2, 26972),
    Token("e", "e", 2, 26973),
    Token(". ", ".", 2, 26974),
    Token("Florida ", "florida", 2, 26975),
    Token("squatters", "squatters", 2, 26976),
    Token(") ", ")", 2, 26977),
    Token("added", "added", 2, 26978),
    Token(", ", ",", 2, 26979),
    Token("\"", "\"", 2, 26980),
    Token("we ", "we", 2, 26981),
    Token("select ", "select", 2, 26982),
    Token("the ", "the", 2, 26983),
    Token("black ", "black", 2, 26984),
    Token("members ", "members", 2, 26985),
    Token("of ", "of", 2, 26986),
    Token("a ", "a", 2, 26987),
    Token("litter ", "litter", 2, 26988),
    Token("for ", "for", 2, 26989),
    Token("raising", "raising", 2, 26990),
    Token(", ", ",", 2, 26991),
    Token("as ", "as", 2, 26992),
    Token("they ", "they", 2, 26993),
    Token("alone ", "alone", 2, 26994),
    Token("have ", "have", 2, 26995),
    Token("a ", "a", 2, 26996),
    Token("good ", "good", 2, 26997),
    Token("chance ", "chance", 2, 26998),
    Token("of ", "of", 2, 26999),
    Token("living", "living", 2, 27000),
    Token(".", ".", 2, 27001),
    Token("\" ", "\"", 2, 27002)
  ),
  List(
    Token("plants", "plants", 3, 40037),
    Token(", ", ",", 3, 40038),
    Token("whilst ", "whilst", 3, 40039),
    Token("dark", "dark", 3, 40040),
    Token("-", "-", 3, 40041),
    Token("coloured ", "coloured", 3, 40042),
    Token("individuals ", "individuals", 3, 40043),
    Token("escape", "escape", 3, 40044),
    Token(": ", ":", 3, 40045),
    Token("Professor ", "professor", 3, 40046),
    Token("Wyman ", "wyman", 3, 40047),
    Token("has ", "has", 3, 40048),
    Token("recently ", "recently", 3, 40049),
    Token("communicated ", "communicated", 3, 40050),
    Token("to ", "to", 3, 40051),
    Token("me ", "me", 3, 40052),
    Token("a ", "a", 3, 40053),
    Token("good ", "good", 3, 40054),
    Token("illustration ", "illustration", 3, 40055),
    Token("of ", "of", 3, 40056),
    Token("this ", "this", 3, 40057),
    Token("fact", "fact", 3, 40058),
    Token("; ", ";", 3, 40059),
    Token("on ", "on", 3, 40060),
    Token("asking ", "asking", 3, 40061),
    Token("some ", "some", 3, 40062),
    Token("farmers ", "farmers", 3, 40063),
    Token("in ", "in", 3, 40064),
    Token("Florida ", "florida", 3, 40065),
    Token("how ", "how", 3, 40066),
    Token("it ", "it", 3, 40067),
    Token("was ", "was", 3, 40068),
    Token("that ", "that", 3, 40069),
    Token("all ", "all", 3, 40070),
    Token("their ", "their", 3, 40071),
    Token("pigs ", "pigs", 3, 40072),
    Token("were ", "were", 3, 40073),
    Token("black", "black", 3, 40074),
    Token(", ", ",", 3, 40075),
    Token("they ", "they", 3, 40076),
    Token("informed ", "informed", 3, 40077),
    Token("him ", "him", 3, 40078),
    Token("that ", "that", 3, 40079),
    Token("the ", "the", 3, 40080),
    Token("pigs ", "pigs", 3, 40081),
    Token("ate ", "ate", 3, 40082),
    Token("the ", "the", 3, 40083),
    Token("paint", "paint", 3, 40084),
    Token("-", "-", 3, 40085),
    Token("root ", "root", 3, 40086),
    Token("(", "(", 3, 40087),
    Token("Lachnanthes", "lachnanthes", 3, 40088),
    Token(")", ")", 3, 40089),
    Token(", ", ",", 3, 40090),
    Token("which ", "which", 3, 40091),
    Token("coloured ", "coloured", 3, 40092),
    Token("their ", "their", 3, 40093),
    Token("bones ", "bones", 3, 40094),
    Token("pink", "pink", 3, 40095),
    Token(", ", ",", 3, 40096),
    Token("and ", "and", 3, 40097),
    Token("which ", "which", 3, 40098),
    Token("caused ", "caused", 3, 40099),
    Token("the ", "the", 3, 40100),
    Token("hoofs ", "hoofs", 3, 40101),
    Token("of ", "of", 3, 40102),
    Token("all ", "all", 3, 40103),
    Token("but ", "but", 3, 40104),
    Token("the ", "the", 3, 40105),
    Token("black ", "black", 3, 40106),
    Token("varieties ", "varieties", 3, 40107),
    Token("to ", "to", 3, 40108),
    Token("drop ", "drop", 3, 40109),
    Token("off", "off", 3, 40110),
    Token("; ", ";", 3, 40111),
    Token("and ", "and", 3, 40112),
    Token("one ", "one", 3, 40113),
    Token("of ", "of", 3, 40114),
    Token("the ", "the", 3, 40115),
    Token("\"", "\"", 3, 40116),
    Token("crackers", "crackers", 3, 40117),
    Token("\" ", "\"", 3, 40118),
    Token("( ", "(", 3, 40119),
    Token("i", "i", 3, 40120),
    Token(".", ".", 3, 40121),
    Token("e", "e", 3, 40122),
    Token(". ", ".", 3, 40123),
    Token("Florida ", "florida", 3, 40124),
    Token("squatters", "squatters", 3, 40125),
    Token(") ", ")", 3, 40126),
    Token("added", "added", 3, 40127),
    Token(", ", ",", 3, 40128),
    Token("\"", "\"", 3, 40129),
    Token("we ", "we", 3, 40130),
    Token("select ", "select", 3, 40131),
    Token("the ", "the", 3, 40132),
    Token("black ", "black", 3, 40133),
    Token("members ", "members", 3, 40134),
    Token("of ", "of", 3, 40135),
    Token("a ", "a", 3, 40136),
    Token("litter ", "litter", 3, 40137),
    Token("for ", "for", 3, 40138),
    Token("raising", "raising", 3, 40139),
    Token(", ", ",", 3, 40140),
    Token("as ", "as", 3, 40141),
    Token("they ", "they", 3, 40142),
    Token("alone ", "alone", 3, 40143),
    Token("have ", "have", 3, 40144),
    Token("a ", "a", 3, 40145),
    Token("good ", "good", 3, 40146),
    Token("chance ", "chance", 3, 40147),
    Token("of ", "of", 3, 40148),
    Token("living", "living", 3, 40149),
    Token(".", ".", 3, 40150),
    Token("\" ", "\"", 3, 40151)
  )
)
def areTokensSequential(input: List[Token]): Boolean =
  input
    .sliding(2, 1) // pairs of consecutive tokens in reading
    .map(e => e.head.g + 1 == e.last.g) // positions should also be consecutive
    .forall(e => e) // for all pairs

@main
def mexico(): Unit = // bittersweet goofy method name
  val result = data.map(areTokensSequential) // check each witness separately
  result.foreach(println)
